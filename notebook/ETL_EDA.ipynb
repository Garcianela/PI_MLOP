{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 59333\n",
      "Número de columnas: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Iniciamos abriendo el archivo JSON con el objetivo de transformarlo en un dataframe.\n",
    "with open('australian_user_reviews.json', 'r', encoding='utf-8') as file:\n",
    "    data = [eval(line) for line in file]\n",
    "\n",
    "user_reviews = pd.DataFrame(data)\n",
    "\n",
    "# Ampliamos la columna 'reviews' con el propósito de convertir cada elemento de la lista en una fila\n",
    "user_reviews_exploded = user_reviews.explode('reviews')\n",
    "\n",
    "# Utilizamos la función json_normalize para desplegar los diccionarios contenidos en la columna 'reviews\n",
    "user_reviews_normalized = pd.json_normalize(user_reviews_exploded['reviews'])\n",
    "\n",
    "# Restablecimos el índice en ambas DataFrames.\n",
    "user_reviews_exploded.reset_index(drop=True, inplace=True)\n",
    "user_reviews_normalized.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se busca conservar las columnas originales de 'user_reviews' además de las \n",
    "# nuevas columnas generadas a partir de 'reviews', logrando esto mediante una fusión (merge) basada en el índice.\n",
    "user_reviews_final = pd.concat([user_reviews_exploded.drop('reviews', axis=1), user_reviews_normalized], axis=1)\n",
    "\n",
    "# El tamaño del DataFrame 'user_reviews_final' es el siguiente:\n",
    "num_filas, num_columnas = user_reviews_final.shape\n",
    "\n",
    "# Muestra el tamaño del DataFrame.\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 5170015\n",
      "Número de columnas: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Para analizar los datos del archivo JSON, primero debemos abrirlo y convertirlo en un dataframe.\n",
    "with open('australian_users_items.json', 'r', encoding='utf-8') as file:\n",
    "    data = [eval(line) for line in file]\n",
    "\n",
    "user_items = pd.DataFrame(data)\n",
    "\n",
    "# Para analizar los datos de la columna \"items\", primero debemos expandirla para que cada elemento de la lista sea una fila.\n",
    "user_items_exploded = user_items.explode('items')\n",
    "\n",
    "# Para convertir los diccionarios en la columna \"items\" a series de filas, podemos usar la función json_normalize(). \n",
    "# Esta función convertirá los diccionarios en series de filas, una para cada elemento del diccionario.\n",
    "user_items_normalized = pd.json_normalize(user_items_exploded['items'])\n",
    "\n",
    "# Reiniciamos el índice en ambos DataFrames\n",
    "user_items_exploded.reset_index(drop=True, inplace=True)\n",
    "user_items_normalized.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Para evitar conflictos entre los índices de ambos DataFrames, primero debemos reiniciarlos.\n",
    "user_items_final = pd.concat([user_items_exploded.drop('items', axis=1), user_items_normalized], axis=1)\n",
    "\n",
    "# El número de filas y columnas del dataframe user_items_final es:\n",
    "num_filas, num_columnas = user_items_final.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 120445\n",
      "Número de columnas: 19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Declarar una variable de tipo list para almacenar los objetos JSON individuales.\n",
    "data = []\n",
    "\n",
    "# Abrir el archivo JSON y cargar cada objeto JSON por separado\n",
    "with open('output_steam_games.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)\n",
    "\n",
    "# Convertir la lista de objetos JSON en un DataFrame\n",
    "steam_games = pd.DataFrame(data)\n",
    "\n",
    "# El tamaño del dataframe user_items_final:\n",
    "num_filas, num_columnas = steam_games.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publisher          96362\n",
      "genres             91593\n",
      "app_name           88312\n",
      "title              90360\n",
      "url                88310\n",
      "release_date       90377\n",
      "tags               88473\n",
      "reviews_url        88312\n",
      "discount_price    120220\n",
      "specs              88980\n",
      "price              89687\n",
      "early_access       88310\n",
      "id                 88312\n",
      "metascore         117768\n",
      "developer          91609\n",
      "user_id            32135\n",
      "steam_id           32135\n",
      "items              32135\n",
      "items_count        32135\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generar un resumen de los datos nulos o faltantes en cada columna.\n",
    "datos_nulos = steam_games.isnull().sum()\n",
    "\n",
    "# Imprime la cantidad de datos nulos por columna\n",
    "print(datos_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 74837\n",
      "Número de columnas: 19\n"
     ]
    }
   ],
   "source": [
    "# Se eliminan los datos faltantes de la columna \"url\", ya que esta columna es irrelevante y posee más del 90% de datos faltantes.\n",
    "steam_games = steam_games.dropna(subset=['url'])\n",
    "\n",
    "# Se renombro la columna 'id' a 'item_id'\n",
    "steam_games.rename(columns={'id': 'item_id'}, inplace=True)\n",
    "\n",
    "# Aplana la columna \"genres\" del DataFrame \"steam_games\".\n",
    "steam_games = steam_games.explode('genres', ignore_index=True)\n",
    "\n",
    "# Se verifica el tamaño del dataframe steam_games.\n",
    "num_filas, num_columnas = steam_games.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 10978\n",
      "Número de columnas: 2\n"
     ]
    }
   ],
   "source": [
    "# Se agrupan los datos de las columnas 'item_id' con el fin de calcular la suma de la columna 'playtime_forever'.\n",
    "\n",
    "grouped_user_items = user_items_final.groupby('item_id')['playtime_forever'].sum().reset_index()\n",
    "\n",
    "# Se renombro la columna de playtime_forever.\n",
    "grouped_user_items.rename(columns={'playtime_forever': 'sum_playtime_forever'}, inplace=True)\n",
    "\n",
    "# El tamaño del nuevo dataframe grouped_user_items:\n",
    "num_filas, num_columnas = grouped_user_items.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame.\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 52781\n",
      "Número de columnas: 10\n"
     ]
    }
   ],
   "source": [
    "# Se concatenan los DataFrames grouped_user_items con user_reviews con el objetivo de tener toda la información en una sola tabla.\n",
    "merged_df = pd.merge( grouped_user_items, user_reviews_final, on='item_id', how='inner')\n",
    "\n",
    "\n",
    "# El tamaño del nuevo dataframe grouped_user_items:\n",
    "num_filas, num_columnas = merged_df.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 118376\n",
      "Número de columnas: 28\n"
     ]
    }
   ],
   "source": [
    "# Se concatenan los dataframes merged_df con steam_games con la finalidad de poseer la informacion en una sola tabla\n",
    "merged_final = pd.merge( merged_df, steam_games, on='item_id', how='inner')\n",
    "\n",
    "# El tamaño del nuevo dataframe grouped_user_items:\n",
    "num_filas, num_columnas = merged_final.shape\n",
    "\n",
    "# Imprime el tamaño del DataFrame\n",
    "print(f\"Número de filas: {num_filas}\")\n",
    "print(f\"Número de columnas: {num_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['item_id', 'sum_playtime_forever', 'user_id_x', 'user_url', 'funny',\n",
      "       'posted', 'last_edited', 'helpful', 'recommend', 'review', 'publisher',\n",
      "       'genres', 'app_name', 'title', 'url', 'release_date', 'tags',\n",
      "       'reviews_url', 'discount_price', 'specs', 'price', 'early_access',\n",
      "       'metascore', 'developer', 'user_id_y', 'steam_id', 'items',\n",
      "       'items_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Se validan los nombres de las columnas para luego eliminar aquellas que se consideren irrelevantes para la creación de los endpoints y el sistema de recomendación.\n",
    "\n",
    "nombres_columnas = merged_final.columns\n",
    "print(nombres_columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes eliminar las columnas especificadas de la siguiente manera:\n",
    "\n",
    "columnas_a_eliminar = ['user_id_y', 'user_url', 'funny', 'last_edited', 'helpful', 'publisher', 'discount_price', 'specs', 'price', 'early_access', 'metascore', 'items_count','items','url','reviews_url']\n",
    "merged_final= merged_final.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "\n",
    "# Se renombra la columna 'user_id_y' a 'user_id' de la siguiente manera:\n",
    "\n",
    "merged_final = merged_final.rename(columns={'user_id_x': 'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de datos de las columnas 'posted'.\n",
    "\n",
    "import re\n",
    "\n",
    "# Se cuenta con un DataFrame llamado 'merged_final'\n",
    "\n",
    "# Define una función para extraer la fecha usando regex\n",
    "def extract_date(text):\n",
    "    match = re.search(r'\\w+ \\d{1,2}, \\d{4}', text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "\n",
    "# Aplica la función solo a las filas donde 'posted' no sea de tipo Timestamp\n",
    "mask = merged_final['posted'].apply(lambda x: isinstance(x, str))\n",
    "merged_final.loc[mask, 'posted'] = merged_final.loc[mask, 'posted'].apply(lambda x: extract_date(x))\n",
    "\n",
    "# Convierte la columna 'posted' a formato de fecha\n",
    "merged_final['posted'] = pd.to_datetime(merged_final['posted'], format='%B %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id                      0\n",
      "sum_playtime_forever         0\n",
      "user_id                      0\n",
      "posted                   21786\n",
      "recommend                    0\n",
      "review                       0\n",
      "genres                    1963\n",
      "app_name                     0\n",
      "title                     1783\n",
      "release_date              1915\n",
      "tags                         0\n",
      "developer                 2130\n",
      "steam_id                118376\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Se procede a validar la cantidad de datos faltantes dentro del dataframe resultante para realizar imputación\n",
    "\n",
    "# Se valida los datos nulos de las columnas\n",
    "nulos_por_columna = merged_final.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizaremos la imputación de los valores faltantes en las columnas del DataFrame.\n",
    "\n",
    "# Para las columnas 'genres' y 'title', que contienen información de tipo texto, utilizaremos \"sin dato\" como mensaje descriptivo.\n",
    "\n",
    "merged_final['genres'].fillna(\"sin dato\", inplace=True)\n",
    "merged_final['title'].fillna(\"sin dato\", inplace=True)\n",
    "merged_final['developer'].fillna(\"sin dato\", inplace=True)\n",
    "\n",
    "# Imputación de datos de las columnas user_id y steam_id, para ello se asigno un valor unico -1\n",
    "\n",
    "merged_final['user_id'].fillna(-1, inplace=True)\n",
    "merged_final['steam_id'].fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Las columnas 'release_date' y 'posted', contienen información de tipo date, se considero imputar calculando el valor máximo de la columna release_date de acuerdo al item_id\n",
    "\n",
    "# Agrupa por 'item_id' y encuentra el valor máximo de 'posted' para cada grupo\n",
    "max_posted_by_item = merged_final.groupby('item_id')['release_date'].max()\n",
    "\n",
    "# Si deseas convertirlo en un diccionario para acceder al valor máximo de 'posted' por 'item_id', puedes hacerlo de esta manera:\n",
    "max_posted_dict = max_posted_by_item.to_dict()\n",
    "\n",
    "# Reemplaza los valores faltantes en 'posted' con el valor máximo correspondiente según 'item_id'\n",
    "merged_final['release_date'].fillna(merged_final['item_id'].map(max_posted_dict), inplace=True)\n",
    "\n",
    "\n",
    "# Finalmente me estan quedando 1599 valores sin reemplazar. Que fueron reemplazados por el valor modal de la columna \n",
    "\n",
    "# Limpiar la columna 'release_date' eliminando valores no válidos\n",
    "merged_final['release_date'] = pd.to_datetime(merged_final['release_date'], errors='coerce', format=\"%Y-%m-%d\")\n",
    "\n",
    "# Calcular la moda de la columna 'release_date' sin eliminar los valores nulos\n",
    "moda_release_date = merged_final['release_date'].mode()\n",
    "\n",
    "# Obtener la primera moda (en caso de que haya múltiples modas)\n",
    "moda_release_date_primera = moda_release_date.iloc[0]\n",
    "\n",
    "# Reemplazar los valores faltantes en 'release_date' con la moda\n",
    "merged_final['release_date'].fillna(moda_release_date_primera, inplace=True)\n",
    "\n",
    "\n",
    "# Imputado de la columna 'posted', contienen información de tipo date, se considero imputar calculando el valor máximo de la columna release_date de acuerdo al item_id\n",
    "\n",
    "# Agrupa por 'item_id' y encuentra el valor máximo de 'posted' para cada grupo\n",
    "max_posted_by_item = merged_final.groupby('item_id')['posted'].max()\n",
    "\n",
    "# Si deseas convertirlo en un diccionario para acceder al valor máximo de 'posted' por 'item_id', puedes hacerlo de esta manera:\n",
    "max_posted_dict = max_posted_by_item.to_dict()\n",
    "\n",
    "# Reemplaza los valores faltantes en 'posted' con el valor máximo correspondiente según 'item_id'\n",
    "merged_final['posted'].fillna(merged_final['item_id'].map(max_posted_dict), inplace=True)\n",
    "\n",
    "\n",
    "# Finalmente me estan quedando 1723 valores sin reemplazar. Que fueron reemplazados por el valor modal de la columna \n",
    "\n",
    "# Limpiar la columna 'release_date' eliminando valores no válidos\n",
    "merged_final['posted'] = pd.to_datetime(merged_final['posted'], errors='coerce', format=\"%Y-%m-%d\")\n",
    "\n",
    "# Calcular la moda de la columna 'release_date' sin eliminar los valores nulos\n",
    "moda_release_date = merged_final['posted'].mode()\n",
    "\n",
    "# Obtener la primera moda (en caso de que haya múltiples modas)\n",
    "moda_release_date_primera = moda_release_date.iloc[0]\n",
    "\n",
    "# Reemplazar los valores faltantes en 'release_date' con la moda\n",
    "merged_final['posted'].fillna(moda_release_date_primera, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_analysis\n",
      "2    75004\n",
      "1    23298\n",
      "0    20074\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Se realiza el análisis de sentimiento de la columna reviews\n",
    "\n",
    "# Descargar el léxico VADER si aún no está descargado\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Crear una instancia del analizador de sentimiento VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Función para asignar un valor de sentimiento según la escala dada\n",
    "def assign_sentiment(row):\n",
    "    # Realizar el análisis de sentimiento en el texto de la columna 'review'\n",
    "    sentiment_scores = analyzer.polarity_scores(row['review'])\n",
    "    \n",
    "    # Calcular un puntaje de sentimiento compuesto\n",
    "    sentiment_score = sentiment_scores['compound']\n",
    "    \n",
    "    # Asignar valores según la escala\n",
    "    if sentiment_score < -0.05:\n",
    "        return 0  # Sentimiento malo\n",
    "    elif sentiment_score >= -0.05 and sentiment_score <= 0.05:\n",
    "        return 1  # Sentimiento neutral o sin dato\n",
    "    else:\n",
    "        return 2  # Sentimiento positivo\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame y crear la columna 'sentiment_analysis'\n",
    "merged_final['sentiment_analysis'] = merged_final.apply(assign_sentiment, axis=1)\n",
    "\n",
    "# Eliminar la columna 'review'\n",
    "merged_final = merged_final.drop('review', axis=1)\n",
    "\n",
    "# Contar los registros por valor en la columna 'sentiment_analysis'\n",
    "sentiment_counts = merged_final['sentiment_analysis'].value_counts()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id                         object\n",
      "sum_playtime_forever           float64\n",
      "user_id                         object\n",
      "posted                  datetime64[ns]\n",
      "recommend                       object\n",
      "genres                          object\n",
      "app_name                        object\n",
      "title                           object\n",
      "release_date            datetime64[ns]\n",
      "tags                            object\n",
      "developer                       object\n",
      "steam_id                         int64\n",
      "sentiment_analysis               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se visualiza el tipo de dato de las columnas de la siguiente manera:\n",
    "\n",
    "# Se corrobora el tipo de dato de las columnas del dataframe\n",
    "tipos_de_dato = merged_final.dtypes\n",
    "\n",
    "# Se Imprime los tipos de datos\n",
    "print(tipos_de_dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte la columna 'recommend' a tipo booleano\n",
    "merged_final['recommend'] = merged_final['recommend'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id                 0\n",
      "sum_playtime_forever    0\n",
      "user_id                 0\n",
      "posted                  0\n",
      "recommend               0\n",
      "genres                  0\n",
      "app_name                0\n",
      "title                   0\n",
      "release_date            0\n",
      "tags                    0\n",
      "developer               0\n",
      "steam_id                0\n",
      "sentiment_analysis      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Se procede a validar la cantidad de datos faltantes dentro del dataframe resultante para realizar imputación\n",
    "\n",
    "# Se valida los datos nulos de las columnas\n",
    "nulos_por_columna = merged_final.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Se crea y valida que las columnas que involucran fecha se encuentren en el formato correcto\n",
    "\n",
    "# Convierte la columna 'release_date' y 'posted' al tipo de dato datetime\n",
    "merged_final['release_date'] = pd.to_datetime(merged_final['release_date'], errors='coerce', format=\"%B %d, %Y\")\n",
    "merged_final['posted'] = pd.to_datetime(merged_final['posted'], errors='coerce', format=\"%B %d, %Y\")\n",
    "\n",
    "# Verifica si hay errores en la conversión\n",
    "print(merged_final['release_date'].isnull().sum())  # Debería imprimir 0 si no hay errores\n",
    "print(merged_final['posted'].isnull().sum())       # Debería imprimir 0 si no hay errores\n",
    "\n",
    "# Crea la columna 'anio_release' que contiene el año de 'release_date'\n",
    "merged_final['anio_release'] = merged_final['release_date'].dt.year\n",
    "\n",
    "# Crea la columna 'anio_posted' que contiene el año de 'posted'\n",
    "merged_final['anio_posted'] = merged_final['posted'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea dataset para ser empleada en el primer endpoints que se consumirán en la API\n",
    "# se consideraron las columnas de genres (genero), sum_playtime_forever (horas jugadas), anio_release (año de lanzamiento)\n",
    "\n",
    "dataset_f1=merged_final[[\"genres\", \"sum_playtime_forever\", \"anio_release\"]]\n",
    "\n",
    "# Se pasa el dataframe creado a csv:\n",
    "\n",
    "dataset_f1.to_csv('dataset_f1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea dataset para ser empleada en el segundo endpoints que se consumirán en la API\n",
    "# se consideraron las columnas de genres (genero), sum_playtime_forever (horas jugadas), anio_posted (año de posteo, se considera el año que jugo el usuario), user_id (identificador unico de usuario)\n",
    "\n",
    "dataset_f2=merged_final[[\"genres\", \"sum_playtime_forever\", \"anio_posted\",\"user_id\"]]\n",
    "\n",
    "# Se pasa el dataframe creado a csv:\n",
    "\n",
    "dataset_f2.to_csv('dataset_f2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea dataset para ser empleada en el tercero y cuarto endpoints que se consumirán en la API\n",
    "# se consideraron las columnas de genres\n",
    "\n",
    "dataset_f3=merged_final[[\"user_id\", \"app_name\", \"anio_posted\",\"recommend\"]]\n",
    "\n",
    "# Se pasa el dataframe creado a csv:\n",
    "\n",
    "dataset_f3.to_csv('dataset_f34.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea dataset para ser empleada en el quinto endpoints que se consumirán en la API\n",
    "# se consideraron las columnas de genres\n",
    "\n",
    "dataset_f5=merged_final[[\"sentiment_analysis\", \"anio_release\"]]\n",
    "\n",
    "# Se pasa el dataframe creado a csv:\n",
    "\n",
    "dataset_f5.to_csv('dataset_f5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de duplicados en la columna \"title\": 99.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_10120\\3551264269.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_model.drop_duplicates(subset=\"title\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Se crea dataset con las columnas necesarias para el modelo de machine learning.\n",
    "\n",
    "dataset_model=merged_final[[\"title\", \"user_id\",\"tags\",\"genres\",\"recommend\"]]\n",
    "\n",
    "\n",
    "# Se calcula el porcentaje de duplicados en la columna \"title\"\n",
    "duplicados = dataset_model['title'].duplicated(keep=False)\n",
    "porcentaje_duplicados = (duplicados.sum() / len(dataset_model)) * 100\n",
    "\n",
    "# 'duplicados' es una serie booleana que indica qué filas son duplicadas\n",
    "# 'porcentaje_duplicados' calcula el porcentaje de duplicados con respecto al total de filas en el DataFrame\n",
    "\n",
    "print(f'Porcentaje de duplicados en la columna \"title\": {porcentaje_duplicados:.2f}%')\n",
    "\n",
    "# Se elimina los duplicado de la columna title con la finalidad de reducir recursos computacionales y mejorar recursos computacionales para el modelo\n",
    "dataset_model.drop_duplicates(subset=\"title\", inplace=True)\n",
    "\n",
    "dataset_model\n",
    "\n",
    "# Se pasa el dataframe creado a csv:\n",
    "\n",
    "dataset_model.to_csv('modelo.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
